{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/mukhtarrabayev/Library/Python/3.11/lib/python/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/mukhtarrabayev/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/mukhtarrabayev/Library/Python/3.11/lib/python/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.7 requires protobuf<4,>=3.11, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-25.3.0 outcome-1.3.0.post0 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /Users/mukhtarrabayev/Library/Python/3.11/lib/python/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->webdriver-manager) (2023.5.7)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # Extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "                except:\n",
    "                    composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22component%22%3A%22search%22,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22zigi%20zagi%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.9.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into Render PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Render PostgreSQL connection details (Update this!)\n",
    "DB_URL = \"postgresql://quramdb2:9P3RoNtzfA08JVXClmUgTXE1fH3D7Ys8@dpg-cuua60qj1k6c73dojbt0-a.oregon-postgres.render.com/quramdb2\"\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"/Users/mukhtarrabayev/Downloads/arbuz_products.csv\"\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")  # Use utf-8-sig to avoid encoding issues\n",
    "\n",
    "# Connect to Render PostgreSQL\n",
    "conn = psycopg2.connect(DB_URL)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insert data into PostgreSQL\n",
    "for index, row in df.iterrows():\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO product (name, image, ingredients, status)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row[\"name\"], row[\"image\"], row[\"ingredients\"], row[\"status\"])\n",
    "    )\n",
    "\n",
    "# Commit and close connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into Render PostgreSQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection details for Render\n",
    "DB_URL = \"postgresql://quramdb2:9P3RoNtzfA08JVXClmUgTXE1fH3D7Ys8@dpg-cuua60qj1k6c73dojbt0-a.oregon-postgres.render.com/quramdb2\"\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"/Users/mukhtarrabayev/Downloads/arbuz_products.csv\"\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Extract last part of category (to match your descriptions)\n",
    "df[\"category\"] = df[\"category\"].apply(lambda x: x.split(\">\")[-1].strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(DB_URL)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# 1. Insert unique categories into description table & retrieve IDs\n",
    "category_to_id = {}  # Dictionary to store category → id mapping\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    # Check if the category already exists\n",
    "    cur.execute(\"SELECT id FROM description WHERE name = %s\", (category,))\n",
    "    result = cur.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        category_id = result[0]  # Get existing ID\n",
    "    else:\n",
    "        # Insert the category and get the new ID\n",
    "        cur.execute(\"INSERT INTO description (name) VALUES (%s) RETURNING id\", (category,))\n",
    "        category_id = cur.fetchone()[0]\n",
    "    \n",
    "    category_to_id[category] = category_id  # Store category_id in dict\n",
    "\n",
    "# Commit category insertions\n",
    "conn.commit()\n",
    "\n",
    "# 2. Insert products into the product table with description_id\n",
    "for index, row in df.iterrows():\n",
    "    category_id = category_to_id[row[\"category\"]]  # Get category_id from dictionary\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO product (name, image, ingredients, status, description_id)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row[\"name\"], row[\"image\"], row[\"ingredients\"], row[\"status\"], category_id)\n",
    "    )\n",
    "\n",
    "# Commit product insertions\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection details\n",
    "DB_URL = \"postgresql://quramdb3:cUaVicWuj17LnZDz5a0wCzd6UVzvxZKa@dpg-cvighqqdbo4c73cklfr0-a.oregon-postgres.render.com/quramdb3\"\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"/Users/mukhtarrabayev/Downloads/arbuz_products.csv\"\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Process the 'category' column to get only the last part (cleaned category)\n",
    "df[\"category\"] = df[\"category\"].apply(lambda x: x.split(\">\")[-1].strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(DB_URL)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Dictionary to store category → id mapping\n",
    "category_to_id = {}\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    # Convert category to lowercase to avoid case-sensitive duplication\n",
    "    category = category.lower()\n",
    "\n",
    "    # Check if the category already exists (case-insensitive)\n",
    "    cur.execute(\"SELECT id FROM description WHERE LOWER(name) = LOWER(%s)\", (category,))\n",
    "    result = cur.fetchone()\n",
    "\n",
    "    if result:\n",
    "        category_id = result[0]  # Get existing ID\n",
    "    else:\n",
    "        # Insert the category in lowercase and get the new ID\n",
    "        cur.execute(\"INSERT INTO description (name) VALUES (%s) RETURNING id\", (category,))\n",
    "        category_id = cur.fetchone()[0]\n",
    "\n",
    "    category_to_id[category] = category_id  # Store mapping\n",
    "\n",
    "# Commit category insertions\n",
    "conn.commit()\n",
    "\n",
    "# 2. Insert products into the product table with description_id\n",
    "for index, row in df.iterrows():\n",
    "    category_id = category_to_id[row[\"category\"].lower()]  # Ensure consistent lowercase mapping\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO product (name, image, ingredients, status, description_id)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row[\"name\"], row[\"image\"], row[\"ingredients\"], row[\"status\"], category_id)\n",
    "    )\n",
    "\n",
    "# Commit product insertions\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_kompotai.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # Extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "                except:\n",
    "                    composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22компотай%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_kompotai.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_kompotai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_flint.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # Extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "                except:\n",
    "                    composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22flint%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_flint.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_flint.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_flint2.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # First method to extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "\n",
    "                    # If ingredients contain \"K товарам бренда\", fall back to second method\n",
    "                    if \"K товарам бренда\" in composition_text:\n",
    "                        raise Exception(\"Fallback to second method\")\n",
    "                except Exception:\n",
    "                    # Second method to extract ingredients from different div\n",
    "                    try:\n",
    "                        composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'product-card-description')]//div[contains(@class, 'information')]\")\n",
    "                        composition_text = composition_element.text.strip()\n",
    "                    except:\n",
    "                        composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22flint%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_flint2.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_flint2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_flint3.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # First method to extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "\n",
    "                    # Check if the result looks like a valid ingredient list\n",
    "                    if \"K товарам бренда\" in composition_text or len(composition_text.split()) < 5:\n",
    "                        raise Exception(\"Invalid ingredients, fallback to second method\")\n",
    "                except Exception as e:\n",
    "                    # If first method fails or returns invalid data, fallback to second method\n",
    "                    try:\n",
    "                        composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'product-card-description')]//div[contains(@class, 'information')]\")\n",
    "                        composition_text = composition_element.text.strip()\n",
    "                    except:\n",
    "                        composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22flint%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_flint3.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_flint3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_flint4.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # First method to extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "\n",
    "                    # Check if the result looks like a valid ingredient list\n",
    "                    if \"K товарам бренда\" in composition_text or len(composition_text.split()) < 5:\n",
    "                        raise Exception(\"Invalid ingredients, fallback to second method\")\n",
    "                except Exception as e:\n",
    "                    # If first method fails or returns invalid data, fallback to second method\n",
    "                    try:\n",
    "                        composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'product-card-description')]//div[contains(@class, 'information')]\")\n",
    "                        composition_text = composition_element.text.strip()\n",
    "                    except:\n",
    "                        composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract ingredients starting from the word 'Состав:'\n",
    "                if \"Состав:\" in composition_text:\n",
    "                    composition_text = composition_text.split(\"Состав:\")[1].strip()\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22flint%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_flint4.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_flint4.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection details\n",
    "DB_URL = \"postgresql://quramdb3:cUaVicWuj17LnZDz5a0wCzd6UVzvxZKa@dpg-cvighqqdbo4c73cklfr0-a.oregon-postgres.render.com/quramdb3\"\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"/Users/mukhtarrabayev/Downloads/arbuz_products_flint4.csv\"\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Process the 'category' column to get only the last part (cleaned category)\n",
    "df[\"category\"] = df[\"category\"].apply(lambda x: x.split(\">\")[-1].strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(DB_URL)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Dictionary to store category → id mapping\n",
    "category_to_id = {}\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    # Convert category to lowercase to avoid case-sensitive duplication\n",
    "    category = category.lower()\n",
    "\n",
    "    # Check if the category already exists (case-insensitive)\n",
    "    cur.execute(\"SELECT id FROM description WHERE LOWER(name) = LOWER(%s)\", (category,))\n",
    "    result = cur.fetchone()\n",
    "\n",
    "    if result:\n",
    "        category_id = result[0]  # Get existing ID\n",
    "    else:\n",
    "        # Insert the category in lowercase and get the new ID\n",
    "        cur.execute(\"INSERT INTO description (name) VALUES (%s) RETURNING id\", (category,))\n",
    "        category_id = cur.fetchone()[0]\n",
    "\n",
    "    category_to_id[category] = category_id  # Store mapping\n",
    "\n",
    "# Commit category insertions\n",
    "conn.commit()\n",
    "\n",
    "# 2. Insert products into the product table with description_id\n",
    "for index, row in df.iterrows():\n",
    "    category_id = category_to_id[row[\"category\"].lower()]  # Ensure consistent lowercase mapping\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO product (name, image, ingredients, status, description_id)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row[\"name\"], row[\"image\"], row[\"ingredients\"], row[\"status\"], category_id)\n",
    "    )\n",
    "\n",
    "# Commit product insertions\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed! Data saved to arbuz_products_kompotai2.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_product_data(url):\n",
    "    def driver_setup():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver = driver_setup()\n",
    "    all_products = []\n",
    "\n",
    "    try:\n",
    "        # Open the main URL\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "\n",
    "        # Collect product links and names\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-card__title\"))\n",
    "        )\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-card__title\")\n",
    "        product_links = [elem.get_attribute('href') for elem in product_elements]\n",
    "        product_names = [elem.text.strip() for elem in product_elements]\n",
    "\n",
    "        for product_name, product_link in zip(product_names, product_links):\n",
    "            driver.get(product_link)\n",
    "            time.sleep(2)  # Wait for product page to load\n",
    "\n",
    "            try:\n",
    "                # Extract category from breadcrumbs\n",
    "                category_elements = driver.find_elements(By.CSS_SELECTOR, \".breadcrumb-item a\")\n",
    "                product_category = \" > \".join([elem.text for elem in category_elements]) if category_elements else \"Category not found\"\n",
    "\n",
    "                # First method to extract ingredients\n",
    "                try:\n",
    "                    composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'property-wrapper')]//div[contains(@class, 'content')]\")\n",
    "                    composition_text = composition_element.text.strip()\n",
    "\n",
    "                    # Check if the result looks like a valid ingredient list\n",
    "                    if \"K товарам бренда\" in composition_text or len(composition_text.split()) < 5:\n",
    "                        raise Exception(\"Invalid ingredients, fallback to second method\")\n",
    "                except Exception as e:\n",
    "                    # If first method fails or returns invalid data, fallback to second method\n",
    "                    try:\n",
    "                        composition_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'product-card-description')]//div[contains(@class, 'information')]\")\n",
    "                        composition_text = composition_element.text.strip()\n",
    "                    except:\n",
    "                        composition_text = \"Ingredients not found\"\n",
    "\n",
    "                # Extract ingredients starting from the word 'Состав:'\n",
    "                if \"Состав:\" in composition_text:\n",
    "                    composition_text = composition_text.split(\"Состав:\")[1].strip()\n",
    "\n",
    "                # Extract image link\n",
    "                try:\n",
    "                    image_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'gallery-wrapper')]//img[contains(@class, 'image')]\")\n",
    "                    image_link = image_element.get_attribute(\"src\").strip()\n",
    "                except:\n",
    "                    image_link = \"Image not found\"\n",
    "\n",
    "                # Append data\n",
    "                all_products.append([product_name, product_link, image_link, composition_text, product_category, \"halal\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting details for {product_link}: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# URL of the product search page\n",
    "SEARCH_URL = \"https://arbuz.kz/ru/almaty/search/show#/?%5B%7B%22clear%22%3Afalse,%22slug%22%3A%22where%5Bname%5D%5Bc%5D%22,%22value%22%3A%22компотай%22,%22component%22%3A%22search%22%7D%5D\"\n",
    "\n",
    "# Extract data\n",
    "data = extract_product_data(SEARCH_URL)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"link\", \"image\", \"ingredients\", \"category\", \"status\"])\n",
    "df.to_csv(\"/Users/mukhtarrabayev/Downloads/arbuz_products_kompotai2.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Scraping completed! Data saved to arbuz_products_kompotai2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection details\n",
    "DB_URL = \"postgresql://quramdb3:cUaVicWuj17LnZDz5a0wCzd6UVzvxZKa@dpg-cvighqqdbo4c73cklfr0-a.oregon-postgres.render.com/quramdb3\"\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = \"/Users/mukhtarrabayev/Downloads/arbuz_products_kompotai2.csv\"\n",
    "df = pd.read_csv(csv_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Process the 'category' column to get only the last part (cleaned category)\n",
    "df[\"category\"] = df[\"category\"].apply(lambda x: x.split(\">\")[-1].strip().lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(DB_URL)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Dictionary to store category → id mapping\n",
    "category_to_id = {}\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    # Convert category to lowercase to avoid case-sensitive duplication\n",
    "    category = category.lower()\n",
    "\n",
    "    # Check if the category already exists (case-insensitive)\n",
    "    cur.execute(\"SELECT id FROM description WHERE LOWER(name) = LOWER(%s)\", (category,))\n",
    "    result = cur.fetchone()\n",
    "\n",
    "    if result:\n",
    "        category_id = result[0]  # Get existing ID\n",
    "    else:\n",
    "        # Insert the category in lowercase and get the new ID\n",
    "        cur.execute(\"INSERT INTO description (name) VALUES (%s) RETURNING id\", (category,))\n",
    "        category_id = cur.fetchone()[0]\n",
    "\n",
    "    category_to_id[category] = category_id  # Store mapping\n",
    "\n",
    "# Commit category insertions\n",
    "conn.commit()\n",
    "\n",
    "# 2. Insert products into the product table with description_id\n",
    "for index, row in df.iterrows():\n",
    "    category_id = category_to_id[row[\"category\"].lower()]  # Ensure consistent lowercase mapping\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO product (name, image, ingredients, status, description_id)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row[\"name\"], row[\"image\"], row[\"ingredients\"], row[\"status\"], category_id)\n",
    "    )\n",
    "\n",
    "# Commit product insertions\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
